# 物理内存管理

这一章节主要介绍的时物理内存的结构，以及内核空间中和物理页面的映射关系。内存管理是内核中最重要的部分之一，那么以下的内存管理需要处理什么问题呢？

1. 通过某些数据结构管理物理页
2. 在系统正常的运行时通过伙伴系统分配内存空间
3. 如何按需分配小的内存空间
4. vmalloc机制分配不连续内存空间
5. 用户进程的内存空间

在前面其实已经提到了虚拟地址空间的概念了，对于一个进程来说每一个进程都会有它的虚拟内存空间，基本以3：1分配用户地址空间和内核地址空间。同时我们还会知道，物理地址是通过映射的方法和内核地址空间产生关联。**有个比较重要的是，虚拟地址空间理论上要小于CPU最大的理论地址空间。**

对于管理物理地址空间来说，一般有两种方法：统一内存访问（UMA）和非统一内存访问（NUMA）。那么就先从这个开始说起吧。

### UMA and NUMA

我们首先知道一下啥是UMA，如果说在系统中有一个统一的物理存储供所有的CPU均匀地共享，那就是UMA统一内存架构，例如我们使用的电脑基本上就是这样，但是如果是在比较大型的机器上，一个CPU拥有它本地的内存，其他CPU可以通过总线访问其他的CPU的内存，这个就称为非同一内存架构，可以料想这个访问速度肯定比UMA要慢。

为了不失一般性，在理解所谓物理内存的结构时，先从非统一内存访问所要使用的数据结构开始理解。我们可以把一个RAM内存划分为多个节点（node），这个所谓的节点是和处理器挂钩的（由数据结构pg_data_t表示）。在每一个节点中间又需要划分多个区域（zone），一个节点一般是由3个zone组成的（这个可另外配置），一个zone代表的是高内存空间（内核地址空间映射的扩展），一个zone代表DMA操作空间，还有剩下一个是普通使用的内存空间（就是普通的映射用的）。每一个区域又由一个数组构成，数组元素代表了真正的物理页，就是所谓的页框。这个多个节点就是给非同一内存访问使用的，**每个处理器相当于占用了一个node，而UMA模式下就只会使用一个node**。

那么来熟悉一下pg_data_t的一个具体的结构，其中有几个要着重看一下：

```
typedef struct pglist_data { 
	struct zone node_zones[MAX_NR_ZONES];  //zones
	struct zonelist node_zonelists[MAX_ZONELISTS];  //备用的节点，用来处理装不下的情况 
	int nr_zones;  //zone的数目
	struct page *node_mem_map;  //所有节点中所对应的所有物理页
	struct bootmem_data *bdata; //用于启动系统的内存
	unsigned long node_start_pfn;  //UMA下始终为0
	unsigned long node_present_pages; /* total number of physical pages */ 
	unsigned long node_spanned_pages; /* total size of physical page range, including holes */ 
	int node_id;   //node标识符
	struct pglist_data *pgdat_next; //连接下一个node的指针
	wait_queue_head_t kswapd_wait; 
	struct task_struct *kswapd; 
	int kswapd_max_order; 
} pg_data_t;
```

接着是zone的结构：

```
struct zone { 
	/* Fields commonly accessed by the page allocator */ 
	unsigned long pages_min, pages_low, pages_high;
	//这三个标记用作页兑换来使用，如果当前有大于pages_high的页，那么说明该zone是理想的；如果当前的空余页数已经小于等于pages_low，那么就要考虑和磁盘开始兑换;如果低于了pages_low,那就说明这个zone上已经很缺空闲页了.
	unsigned long lowmem_reserve[MAX_NR_ZONES]; //保留页,用来给那些比较紧急的分配事务
	struct per_cpu_pageset pageset[NR_CPUS];  //所谓冷热页序列,后面解释什么是冷热页
	/* * free areas of different sizes */ 
	spinlock_t lock;   // 自旋锁，防止多个处理器读
	struct free_area free_area[MAX_ORDER]; //用来实现伙伴系统
	ZONE_PADDING(_pad1_)
	/* Fields commonly accessed by the page reclaim scanner */ 
	spinlock_t lru_lock;   //同样防止多个处理器进入
	struct list_head active_list;  //标识了所有活跃的页,就是指它经常被访问
	struct list_head inactive_list;  //标识了所有的不活跃的页,就是指它不经常被访问
	unsigned long nr_scan_active; //代表回收内存时扫描的活跃页
	unsigned long nr_scan_inactive;  //和上面的一样,不活跃页数
	unsigned long pages_scanned;  //上一次页面被换出后扫描失败的页面
	/* since last reclaim */
	unsigned long flags; /* zone flags, see below */  //代表这个zone的类型
	/* Zone statistics */ 
	atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];  //存放大量关于该zone的静态信息
	int prev_priority; //
	ZONE_PADDING(_pad2_) 
	/* Rarely used or read-mostly fields */
	wait_queue_head_t * wait_table;   //处理器等待该页面的等待队列
	unsigned long wait_table_hash_nr_entries; 
	unsigned long wait_table_bits;
	/* Discontig memory support fields. */ 
	struct pglist_data *zone_pgdat;   //这个使得node和zone产生了关联
	unsigned long zone_start_pfn; //指示了这个zone的第一个页框的系数
	unsigned long spanned_pages; 
	/* total size, including holes */ 
	unsigned long present_pages; 
	/* amount of memory (excluding holes) */
	/* * rarely used fields: */ 
	char *name; 
} ____cacheline_maxaligned_in_smp;
```

**冷热页队列**

热页指的是一个页面在CPU的cache中,所以它的数据可以更快地被获取;那么相反一个冷页并不在cache中。这个pageset的一项就分为了两个队列，分别为冷队列和热队列。而这个队列中的每一个元素由以下的几个属性标识：

```
struct per_cpu_pages { 
	int count; /* number of pages in the list */ 
	int high; /* high watermark, emptying needed */   //意思是这个队列中的最大值
	int batch; /* chunk size for buddy add/remove */   //一次性加的页数，当冷热页已经不够了，那么需要从系统中重新读取页面加进来，此时就是按照这个大小来存放的
	struct list_head list; /* the list of pages */  //真正接页面的链表
};
```

**页框**

页框是系统内存中最小的单元。在linux的设计中页框的大小要尽可能的小，但是再小的页面也要被几个应用所使用，因此一个页面可能会被分为多个部分。因此真正在实现的时候使用到了联合体。一个物理页可以被页表映射到多个虚拟地址空间处，这样在这个物理页结构中有一个计数器专门记录页框被映射的次数。如果这一个页框被分为许多小部分供多个应用去使用，那么这个计数就代表使用该页框的应用数。页框的结构如下所述：

```
struct page { 
    unsigned long flags; //这个存放了和架构无关的一些标志，这些所谓的标志有很多都是指该页框现在处于的状态，比如该页是否被修改，是否上锁等等这些状态。
    atomic_t _count;  //这个页框的引用次数，如果引用次数为0则说明这个页框现在并没有被使用
    union { 
    	atomic_t _mapcount;  // 这个的值会在反向映射中使用到
    	unsigned int inuse; /* SLUB: Nr of objects */
	}; 
	union { 
		struct { 
			unsigned long private; 
			struct address_space *mapping;  //这个部分是该页框在地址空间的位置
		};
...
	struct kmem_cache *slab;  //slub分配器独有
	struct page *first_page;  //在分配时有这么一种情况，就是可能将多个页框合并分配，那么所有后续页框指向第一页
	}; 
	union {
		pgoff_t index; 	   //和该页框在地址空间的位置有关
		void *freelist;    //slub分配器独有
	}; 
	struct list_head lru;  //用来对不同的页框分类使用
#if defined(WANT_PAGE_VIRTUAL) 
	void *virtual;   //用作高内存区区域的页框
#endif 
};
```

### 页表

页表用来快速地管理大量的地址空间，它是将用户进程的虚拟地址和实际的页框物理地址进行关联。每一个进程都可以拥有自己的页表。2.6内核内存管理分为四级页表，那么虚拟地址就会被划分为五个部分，Offset代表页框中的偏移，PTE代表一级页表中的偏移，PMD代表二级页表中的偏移，PUD代表三级页表中的偏移，PGD代表四级页表中的偏移。最高级页表的基址被存在一个特定的地方。

最后的入口并不只是代表了一根指向实际地址的指针，而是带有了许多额外的信息，使用了一些bit来记录，这些信息通常是和该页有关的控制信息，包括页的读写权限，修改位等。

### 初始化

对于内存的初始化有着很多层意思，比如在引导阶段需要一些空间来帮助系统完成引导，而这些空间在后面将不会再使用。内存初始化作为系统中最为重要的一个部分，它在一些与系统相关的初始化之后立刻就进行了。也就是说pg_data_t这个结构就在这个时候被初始化了。大部分的系统都只有一个node，那么为了保证系统代码的扩展性，系统使用了contig_page_data来管理了所有的系统内存。

#define NODE_DATA(nid)  (&contig_page_data)

注意这个nid就是专门为了NUMA所准备的，但是你可以看到其实都会被映射到一个实例上，那么对于普通的UMA系统，这个nid参数默认为1。

接下来就是系统在最开始对于内存的初始化操作：

系统会使用setup_arch对一些与架构挂钩的属性进行设置（可以看到下面的使用boot allocator的过程），然后使用setup_per_cpu_areas（SMP上）对每个CPU变量进行初始化。当这两步进行完后，就正式对内存单元进行初始化了。build_all_zonelists构造了node和zone所需要的数据结构。（这里小提一下，对于UMA和NUMA，它们使用的同一套操作函数，内部的实现是通过编译时一些宏定义来区分的。）

对于UMA，因为只有一个node，所以这个函数中直接将contig_page_data结构赋给了pg_data_t，而对于NUMA来说就需要多次赋值，构造多个node。然后将pg_data_t作为参数交给build_zonelists在各个zone中建立一个次序。这是什么意思？比如如果系统想申请一片区域从当前的高内存区域中，如果失败了就会去查看下一个普通的内存区域，如果再失败了就会查看DMA区域，如果再失败了就会去别的node中去查看。这就是一个顺序。所以这个函数会在一个node上的zone中建立一个申请顺序的同时，对于存在多个node的情况也会对查找node的顺序进行一定的限制。这一项填充在了pg_data_t的zonelists结构中。那么真正的初始化操作如下代码：

```
static void __init build_zonelists(pg_data_t *pgdat) { 
	int node, local_node; 
	enum zone_type i,j;
	local_node = pgdat->node_id; 
	for (i = 0; i < MAX_NR_ZONES; i++) { 
		struct zonelist *zonelist;
		zonelist = pgdat->node_zonelists + i;  //每个node的zonelist入口
		j = build_zonelists_node(pgdat, zonelist, 0, j);
	... 
} // 这个函数遍历了所有的zone，下面看到针对一个zone而言的初始化

```

```
static int __init build_zonelists_node(pg_data_t *pgdat, struct zonelist *zonelist, int nr_zones, enum zone_type zone_type) { 
	struct zone *zone;
	do {
		zone = pgdat->node_zones + zone_type;   //初始化不同种类的zone
		if (populated_zone(zone)) { 
			zonelist->zones[nr_zones++] = zone; //初始化该node中的zonelist
		} 
		zone_type--;
	} while (zone_type >= 0); 
	return nr_zones;  //其实返回的是种类数
}   // zone_type是按照初始约定好的每一个不同类型区域的顺序，因此在初始化的时候对于单个zone就会按照顺序初始化zonelist中的zone，举个例子，假设在zone_type中顺序是ABC， 则初始化后这个zonelist种的zones数组应该就有ABC三项

```

当我们完成了结构的初始化后就要进行所谓的建立顺序的操作了。

```
static void __init build_zonelists(pg_data_t *pgdat) { 
	... 
	for (node = local_node + 1; node < MAX_NUMNODES; node++) { 
		j = build_zonelists_node(NODE_DATA(node), zonelist, j, i); 
	} 
	for (node = 0; node < local_node; node++) { 
		j = build_zonelists_node(NODE_DATA(node), zonelist, j, i); 
	}
	zonelist->zones[j] = NULL;
	}
} 
} //这里的代码就是循环往后进行拼接，举个例子，如果当前正在初始化的node是2号，如果一共有4个node，那么拼接出的就是2->3->0->1，这就是所谓node的顺序。每个node中的顺序又是按照指定的zone_type拼接的。

```

#### 与架构有关的初始化

（本篇只关注IA-32和AMD64两种架构。）

在真正接触初始化之前，先要了解一些和物理RAM有关的内容。内核真正在物理RAM中是这样分布的：开始的4KB是第一个物理页框，这个通常被BIOS保留。接下去的640KB内容是可用的，但是在内核被被加载后就不会再使用了，因为这片区域之后紧接着就是系统的ROM和显卡。而显卡末尾位置为1MB。那么如果使用这片区域来加载内核则内核的大小一定要小于640KB，IA-32为了解决这个问题就让内核的加载位置放到了1MB后。\_text和\_etext标识了内核代码的开始地址和结束地址。需要的数据部分在\_etext和\_edata中间。而最后到\_end的这篇区域内就包含了其他初始化数据。你可以通过System.map这个文件来查看，也可以通过/proc/iomem来查看。

###### 初始化时的操作

下面主要介绍内核在初始化的时候一些有关内存分配的一系列操作：

machine_specific_memory_setup，这个函数是最先被调用的有关内存分配的函数，这是为了获得建立一个列表哪些内存区域被系统利用而哪些没有（这一步还是很重要的，因为后面对于内存的分配要借助于这个数据）。通常这个信息在BIOS中有写，如果没有指定这样的信息内核就会自己建立一个表格。然后内核使用parse_cmdline_early分析命令行的参数，管理员可以用这个功能来覆写一些已经设置好的有关内存分配的参数。**接下来进入setup_memory，这个函数对于内存连续和内存不连续的机器实现是不一样的。但是它们都可以使用这个函数来决定物理页数，保留的内存空间以及用于引导的内存空间。**随后内核调用paging_init初始化**内核页表**并且允许分页。最后就是调用zone_size_init函数初始化所谓pgdat_t的实例了。以上的操作是在IA-32架构中出现的，如果是AMD64架构的计算机，则初始化的流程也十分相似。下面着重来看这个paging_init这个过程。

首先要知道的一点，这个page_init的作用之一是初始化只有内核能使用的页表。首先一点我们要回忆以下进程的虚拟地址空间，内核会将这个空间分为3GIB的用户地址空间和1GIB的内核地址空间，用户地址空间是每个进程所独有的，但是内核空间而言当通过系统调用陷入了内核态，此时内核处理时必须在一个很稳定安全的地方，那么就需要让内核完全占用一个地址空间，而这个空间可以直接映射到物理页。

此时来看一下地址空间的划分，我们知道0-3G一定是给用户的，而3G-4G是给内核的。但是思考一下如果将内核空间都映射到了物理页上也只会只用物理页的1G空间，**也就是如果物理内存有4G，剩下的3G将永远无法访问**。在内核中就使用了**高端内存**的办法来解决这个问题，高端内存的思想就是借用这一片区域来映射对应的物理内存。其实在真正的机器中是不会映射1G的，只有映射896M，那么还剩128M用来干嘛了？（所以我们回忆在之前讲到物理地址空间分为三种区域，即DMA区域，normal区域和高端区域，像normal区域就是用来和内核逻辑地址进行一一映射用的，而剩下的高端地址就是使用接下来说的vmalloc动态地址映射来进行使用）它们分别使用来进行vmalloc，永久映射和固定映射使用。所谓的高端地址就是从0xF8000000~0xFFFFFFFF。

这时就可以详细说说这个函数是怎么做的了。首先通过pagetable_init函数初始化系统页表。然后将896M的物理地址映射上去。当然完成这一步的同时剩下的128M也初始化完毕了。当这一步完成后，就会初始化cr3寄存器，指向全局页目录。接下来冲刷TLB。最后使用了kmap_init函数初始化一个叫做kmap_pte的变量来指定用来进行将高内存地址映射到内核地址空间的一张页表入口。

当然在初始化的过程中还会初始化pageset这个数据结构

下面来看一下AMD64上是怎么操作的。因为64位的虚拟地址，使得它并不需要借助高位地址的来处理映射关系，这比32位虚拟地址的情况略微简单。但是这样反过来想，目前物理地址最大支持48位，这样使用有些虚拟地址无法用来进行映射。内核中解决这个的办法是使用一种符号映射的方法来处理问题的。具体的方法是地址的0-46位用来进行设置，而47-63位则要么全设为1要么全设为0。这样整个虚拟地址空间被划分为了3个部分：高位内核地址，禁止使用空间和低位用户进程空间。

#### 引导过程的内存管理

在引导阶段，内核也需要一些空间让它能够完成引导。在内核中bootmem allocator这个部分就是用来在引导时为内核分配存储空间的。在内核中使用一个bitmap来管理所有物理页的使用情况，那么这个allocator会扫描这个bitmap来搜索出第一个最合适的连续的页面，因为在这个时候系统更加关注的是操作的简单，所以采用的是最优匹配的策略。内核中会使用一个bootmem_data的数据结构，而这个数据结构占用的空间必须在编译时指定。

```
typedef struct bootmem_data { 
	unsigned long node_boot_start;   //系统中的第一页，通常为0
	unsigned long node_low_pfn;    //物理地址空间最后一页，就是zone_normal的位置
 	void *node_bootmem_map;  //bitmap的地址 
	unsigned long last_offset;  //最后一次分配的页偏移
	unsigned long last_pos;   //最后一次分配的页
	unsigned long last_success;  //指定最后一此分配成功的位置，也是下一次分配的开始位置
	struct list_head list;  //
} bootmem_data_t;
```

那么初始化这样的一个结构是一个和架构有关的过程。在IA-32架构上，它会使用setup_memory函数从低地址区域中检测可以用的最大页框数（这里扫描出来的并不代表是空闲的页面，因为对于内核而言，它无法知道页面是否空闲），那么我们在上面也见识过了，在machine_specific_memory_setup这个步骤中，系统根据物理内存使用情况会建立一个表格，注明了各个页面的使用情况。那么这里就可以借助这个表格，来初始化这个bitmap结构。

在使用这个bootmem分配器分配内存时，它给出了一些类alloc_bootmem这种函数来进行分配。使用完成后使用free_bootmem函数来释放，free_bootmem这个函数就是检测只有当某一个页面完全不需要的时候才会被释放，但是往往会出现一个页面上同时有两个进程可能正在被使用。那么此时在释放时就会有一些难以处理。还好在内核中调用这个函数的可能性并不大，因为使用alloc_bootmem这个函数都是为了分配空间存放内核中需要的数据结构，但是这些数据结构贯穿于整个内核的运行。

一旦系统完成了初始化，那么这个分配器也就要被释放了，接下去系统运行的过程中就会使用伙伴系统，系统会释放所有没有被分配出去的内存空间。

### 伙伴系统

在内核被初始化完成后，真正对物理内存进行管理是借助伙伴系统分配器来完成的。回想一下在之前的zone结构中，专门有一块叫做free_area的数组来实现伙伴系统。

```
struct free_area {
    struct list_head free_list[MIGRATE_TYPE];
    unsigned long nr_free;
}

struct free_area free_area[MAX_ORDER];
```

其中的系数order决定了申请的块大小，在伙伴系统中申请页面，是一次性申请2的order次方的页面数，那么在这个free_area的每一个数组入口中就存放着nr_free个这么大小的块。

#### 禁止分片

首先考虑这样一个问题，在系统已经跑了很长时间之后，势必物理内存中的分配分布情况是很零散的，那么内核是很难去处理这样一个分片的情况，这样可能导致明明物理内存还有很多但是想要映射一个大的区域已经不行了。解决这个问题并不能像文件系统那样简单地进行迁移，所以在内核中采取的措施是禁止分片，为了完成这一个目标，内核中把所有的页面分成了三种类型：无法移动的页面（一般内核会占用），回收页面（可以被移动但是它们可以被删除，一般文件会占用），可移动页面（一般用户应用程序会占用）。

可以看到在free_area中，它将free_list又按照迁移类型划分成了多条链表。那么如果在某一次申请空间时没有办法申请到足够大小的内存块时呢？和之前提到zone中的方法一样，它也有一个fallbacklist的结构，用来留作备用，而这个结构系统中给出了。

有一个有趣的细节是在初始化这个部分的过程中，所有的页面最开始都是确定为可移动的。那么根据之前所说的，在分配指定的迁移类型时如果不够，那么会去后备列表中查找，这样就完成了从可移动到不可移动的转换。

在内存中还有一个技术称为虚拟可移动区域，也就是指定了ZONE_MOVABLE，它会把物理区域分为可移动的和不可移动的两部分。

#### 继续初始化zone和node

在上面介绍的初始化过程中，我们只看到了初始化node中的后备队列，但是关于一个zone中的页面却没有看到初始化的部分。首先我们要知道一点，在上面提到的page_init函数中不仅是指定了一个内核页表，它还做了几个重要步骤：获得了每个zone的边界信息max_zone_pfn，每个node的物理页映射early_node_map，最后做的事调用函数free_area_init_nodes(内部调用了free_area_init_node)函数来给每个node填充free_area区域。

在这个函数里，内核统计一个node中占有的物理页数，然后需要的是初始化在node中的一个结构node_mem_map，当然这个页映射关系也被记录在全局变量mem_map中。

然后对于每一个zone结构进行初始化，使用init_currently_empty_zone初始化free_area结构，所有的free_area都会初始化为可移动的，并且每一项的nr_free都会先被初始化为0，直到bootmem这个结构已经被释放。

#### 伙伴系统中的功能API

当运行时需要分配一定的页面时，要调用alloc_pages(mask, order)函数，这个函数可以分配2的order次方的页面块，像这样的分配页面的函数还有很多，比如get_free_page,get_dma_page等。

首先我们从alloc_pages函数可以看出，除了给出了一个指定页大小的order参数外还有一个mask参数。首先这个掩码可以用来区分这个页面是从哪个zone中取出的，而这个参数默认是指定__GFP_NORMAL，也就是从normal zone中取页面。当然在这个mask上还能指定其他的一些参数，这里就不详细展开了。

那么其实在内核中进行分配页面的函数的声明方式有多种，但是归根到底调用的最底层的函数就是alloc_pages，而这个函数又会调用alloc_pages_node函数，而简单地理解，这个函数又是宏**__alloc_pages**的封装，只不过此时需要获取你是从哪个node上获取页面。下面谈一下伙伴系统中最复杂的如何选择合适的页面来分配：

首先做的第一步，在分配页面时可能会指定一些标志位，而这些标志位会对需要的页面提出要求，系统要有办法确定是否可以满足这些标志。此后调用get_page_from_freelist函数，决定是否可以执行页面的分配，它会判断如果当前的zone中没有足够的页面，那么遍历其他node看是否有足够的，如果有足够的尝试使用buffer_rmqueue函数将该页面从zone中移除。那仔细看看__alloc_pages宏，其实就是使用了get_page_from_freelist来找页面的，但是第一轮的寻找可能并不能返回想要的页面，那第二轮查找前，系统使用kswapd线程实施页面对换策略（后面才会讲到，但是这一次可能还是会失败，如果这个请求硬是要申请页面，那么内核还会再做一次遍历，并且接下去的操作可能导致睡眠（详细的设计不赘述，因为涉及太多标志位，如果有需要再回来看吧）。如果真的没取到页面，则内核会报一个warning并返回一个空指针。

我们假设现在已经拿到合适的页面选项了，那么下一步需要从zone中移除这个页面。buffer_rmqueue要判断是否页面是连续的，如果order为0，则这个页面并不需要到伙伴系统中去取，而是只要到冷热cache中拿就可以，如果冷队列已经不够分配了，就可以从伙伴系统中按照batch值加载页面进入队列中（注意这个分配过程也要查看迁移类型）。order不为0，如果zone中没有连续的满足大小的页面就会返回空指针，否则就开始截取页面了。分两步，首先在当前的迁移队列中找，如果找不到就去别的迁移队列中找。真正找页面的过程中，首先从小到大遍历free_area数组，也就是max-order的范围内，一级一级寻找，如果在指定order大小那一级并没有合适的，就向上查找，直到找到了就将大的页面块对半切割，直到切割到需要的order大小，拿走要的那一部分，不需要的按照order大小挂到指定的数组中。

以上废了那么多话去说了取，接下来看如何释放。释放对应的最底层有一个宏叫__free_page。它要判断两种情况，如果要释放的页面存在于热cache中，则这个页面并不需要归还给伙伴系统，因为经常用吧。但是如果说返还的页面已经超过了pcp的count数量，则就会将页面返还给伙伴系统（按照指标batch）。下面看如果是释放不在热cache中的页面，则内核将寻找相同大小的空闲页面块，和它拼凑成更大的页面块尝试插入上面一级的队列中，而这个空闲页面块的要求就是在原先的位置上和它是连续的，这需要两点保证：伙伴的地址和合并后页面的系数。下面就是一个例子，而每一步合并都需要修改order对应的队列的数据。

![kernel10](..\..\img\kernel10.png)

#### vmalloc

以上谈到的内存的分配都是连续分配的，但是总有分配的时候明明有足够的物理页面但是因为不连续而没法分配，那么这里就给出了一个可以分配非连续页面的方法。

首先看一下要使用的数据结构：

```
struct vm_struct {
    struct vm_struct* next; // 链接section的指针
    void *addr;  // 存放的是虚拟地址空间中的开始地址
    unsigned long size;  // 虚拟地址的长度，虚拟地址在分配时是连续的
    unsigned logn flags;
    struct page **pages;  // 该section所分配的页面
    unsigned int nr_pages;  // 页面数
    unsigned long phys_addr;
}
```

**所以使用vmalloc分配的页面从虚拟地址空间的角度而言是连续的，但是实际物理页面上是不连续的。**

在内核中有一个全局变量vmlist管理着所有的vm_area，要使用它首先要找到一个合适的vm_area实例，它需要遍历这个vmlist结构找到一个合适的位置（如果不用就使用api移除），随后为每一个单独的虚拟地址空间分配一个物理页面，而这个物理页面调用上面所述的alloc_page函数即可。如果需要释放这些页面，则调用vfree，对应底层函数__vunmap，这个函数可以做到移除vm_area结构以及释放其中的物理页。

#### 内核映射

最后谈一下之前在提到系统中内核地址空间中分布时排在最后的两个部分，持久内核映射。

之前提到内核地址空间中并不能实现和物理内存一一映射，因为32位机上内核中的地址实在不够，所以需要使用高端内存来帮助。vmalloc其实可以用来帮助映射，但是它的本质出发点并不是在这里。内核中使用持久内核映射的方法来实现高端内存的映射。

```
struct page_address_map {
    struct page *page;
    void *virtual;
    struct list_head list;
} // 内核中持续映射部分就是一个这个结构的数组
```

它使用上面的结构进行映射，可以看到一个页面对应一个地址，而这个list用来处理地址碰撞。

先说一下怎么用这个结构哦，page_address这个函数帮助我们确定一个页面的虚拟地址，我们知道所有的页面都被存放在了一个全局的结构mem_map中，实际上上面的page就是mem_map的实例，那么如果我们要找的页面是属于normal的，就直接可以取mem_map上查看，否则需要进入list_head中去查找。

咋完成映射的？主要处理的是highmem区域的页面。系统中有一个pkcount用来计数这个映射结构的多少，使用它可以寻找一个空的位置存储映射（如果没有找到的话是可以睡眠的），然后使用set_page_address函数插入到这个数组中，返回页面虚拟地址。使用umkmap函数完成一次释放。

但是这个函数不能用在中断句柄中，因为能够睡眠，所以在内核地址空间的最后加上了一个fixmap机制，保证这个映射是一个原子操作（映射对应一个类型，一个类型对应一个入口）。

### Slab分配器

谈到现在，所有的分配都是以至少一个页面的大小分配的，但是设想，如果我只是为了存储一个很小的字符串分配一个页面，是不是有点大材小用了。slab分配器不仅可以分配小的对象，同时它作为了一个存储对象的cache（并不是硬件中的cache）使用，这样一些常用的对象结构在每次使用的时候就不需要进行繁琐的初始化等操作了，它已经存在于slab管理的缓存当中了。

在内核中使用kmalloc函数为指定size字节大小的区域预留一个地址并返回该地址的起始指针。使用kfree释放指针对应的内存空间。下面来看看具体这个slab分配器是怎么做的，结构与实现：

首先讲一些有的没的，对于这里使用到的cache来说，它可以通过kmem_cache_create来创建一个cache项，使用kmem_cache_free函数释放。好，那就先掌握一下具体的结构，一个cache对象包括了需要管理的数据内容和被管理的slab对象，而每一个cache要做的是管理好一种类型的对象，所有的这些cache都连在了一个双向链表上。所以你要说这个slab分配器的本质就是会和高速缓冲区挂钩，它把所有常用的小的对象放入了高速缓冲区中，这样就可以直接从这个slab分配器中拿对象，释放对象也只是归还到slab分配其中。

![kernel11](..\..\img\kernel11.png)

对于一个slab对象而言，它的结构也是十分复杂，对于对象的大小说明，一般而言它使用空指针的倍数大小的规模来存放，例如如果又6的字节，则使用8个字节来存放它，因为空指针的大小是4个字节。大体的结构就是存在一个slab头部信息，后面跟着slab控制块，控制空闲的对象，而后面的空间用来分配给每一个对象使用。一般而言我们更加关注的是后面的空闲的对象入口，所以slab中将序号最小的那一块对象记录在控制块头部中，然后在接下去的每一个控制块中有指出了下一个空的控制块的系数。

既然对象大小并不能够确定，那么在分配后会腾出一些空间是空闲的但是又无法被分配的，这一片区域就是slab的着色区。着色区保证slab中的每一个对象的起始位置都按照高速缓冲区中的每一个缓存行对齐。对于这个slab控制信息来说，它可以存在在slab内部，也可以存放在slab外部。最后，每一个对象都是持有它的物理地址的，那么这样就可以直接找到页实例了。

#### 实现

先看一下具体用到的数据结构：

**使用到的cache的结构：**

```
struct kmem_cache {
    struct array_cache *array[NR_CPUS];  // 存放的是每一个cpu中的空闲的未使用的slabs
    unsigned int batchcount; // 指定的是从这个slab中的cache中一次性取的对象数
    unsigned int limit; // 限制了在每一个CPU队列中最多可以放多少个对象，如果超过了，就需要从list中按照batchcount的数量返还给slab
    unsigned int shared;
    
    unsigned int buffer_size;  // 指定了当前这个cache中对象的大小
    unsigned int flags;
    unsigned int num;  // 最大数目
    
    unsigned int gfporder;  // 一个slab分配器所占有的页数，也就是从伙伴系统中取出2的gfporder次方
    gfp_t gfpflags;
    size_t colour;
    unsigned int colour_off;  
    struct kmem_cache *slabp_cache;  // 如果slab头部在外部存放，那么这个指针将指向对应的cache项
    unsigned int dflags;
    void (*ctor) (struct kmem_cache *, void *);
    
    const char* name;  // cache的名字
    struct list_head next; // 指向下一个cache
    
    struct kmem_list3 *nodelists[MAX_NUMNODES];  // 三大队列：free，partial，full分别对应的是空闲slab的队列，部分满的队列和全满的队列
}
```

那么每一个CPU如果需要使用这个slab分配器，就会配备一个array_cache的实例，存放slab中的对象：

```
struct array_cache {
	unsigned int avail;  // 这个只表示的是当前在这个cache中存在的对象数
	unsigned int limit;  // 和cache中一样
	unsigned int batchcount; // 和cache中一样
	unsigned int touched;  // 当有一个对象被移除时置为1，cache缩减时置为0
	spinlock_t lock;
	void *entry[];  // 存放对象的指针
}
```

最后的管理slab的队列结构kmem_list3如下：

```
srruct kmem_list3 {
    struct list_head slabs_partial;
    struct list_head slabs_full;
    struct list_head slabs_free;
    unsigned long free_objects;  // 记录了所有空闲的slab的数目
    unsigned int free_limit;  // 指定了最大数量的未使用的对象
    unsigned int colour_next;
    spinlock_t list_lock;
    struct array_cache *shared;
    struct array_cache **alien;
    unsigned long next_reap;
    int free_touched;  // 记录了这个slab是否活跃，当一个slab被取到了cache中，就记为1
}
```

#### 初始化

首先我们可以发现，这个所谓的slab分配器还是**建立在伙伴系统的基础上的**，但是这样存在了一个问题，如果我们初始化这个slab的数据结构，那么最好使用的是kmalloc，但是问题在于kmalloc是要通过slab分配器来操作的。那么在内核中，它使用kmem_cache_init函数进行初始化，当然它的初始化时机一定实在伙伴系统已经初始化完成以后。那么此时在最开始，这个函数需要使用在编译时期预留好的静态数据来初始化这个array_cache结构，然后初始化kmalloc所常用的cache（讲波道理，没怎么看懂，就知道他能这么用吧）。

#### 创建

接下去考虑如何创建一个cache，使用函数kmem_cache_create。首先你得计算可以装下对象的size的大小，也就是计算整体的数据的alignment，然后一个struct kmem_cache实例就建立起来了，接下来考虑是否要将头部信息保存在slab中，如果一个对象的大小比一个页框的1/8大，那就不放在slab中。然后针对一个slab，由于对象的大小已经确定了，因此就可以从伙伴系统中找到合适的页面来建立一个个slab了。接下去是对slab进行着色，着色的目的就是让每一个对象都和每一个cache行对齐。

#### 分配

使用kmem_cache_alloc函数可以从某个特定的cache中获得对象，这个函数和kmalloc类似，最后调用的是____cache_alloc函数，如果说这个对象已经在CPUcache中，则直接从这个cache中拿走就可以了，否则则需要调用函数cache_alloc_refill函数。说明当前CPUcache中已经没有更多对象了，就需要从partial slabs队列和free slabs队列中按照batchcount的大小重新填充cache。那如果说，真的已经没有空闲的对象了怎么办呢？此时必须要扩大cache才能保证有更多的对象，因此调用cache_grow函数（这一段有空再研究研究吧），如果一个被分配的对象不再需要使用了，那它就可以被返还给slab了，调用最底层的__cache_free。如果在释放的过程中，发现在CPU的缓存中还存放的下对象，就直接将该对象返还给这个缓存好了，否则就按照batchcount的数量返还给slab队列。

最后看一下kmalloc到底怎么做的，kmalloc执行到最底层就是调用了上面说的那个____cache_alloc函数，它本身维护一个cache_size的数据结构，使用这个结构静态定义了一系列可以获得的cache的大小的值，在进行kmalloc的时候会遍历所有已经定好的cache大小的值，然后找到一个合适的cache，随后调用cache_alloc函数。